{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы:\n",
    "* Макрушин С.В. Лекция 9: Введение в обработку текста на естественном языке\\\n",
    "* https://realpython.com/nltk-nlp-python/\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "edit_distance(\"pi19-2\", \"пи19-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['облагодетельствованным',\n",
       " 'облагодетельствованных',\n",
       " 'председательствовавший',\n",
       " 'стадвадцатипятирублевой',\n",
       " 'высокопревосходительство',\n",
       " 'высокопревосходительства',\n",
       " 'попреблагорассмотрительст',\n",
       " 'попреблагорассмотрительствующемуся',\n",
       " 'убегающих',\n",
       " 'уменьшившейся']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./09_string_2_data/litw-win_utf8.txt\", \"r\", encoding='utf-8') as f:\n",
    "    words = [line.split()[1] for line in f]\n",
    "words[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'величайшим'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(words, key=lambda w: edit_distance(w, \"велечайшим\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'с величайшим усилием выбравшись из '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = \"\"\n",
    "for word in word_tokenize(text)[:5]:\n",
    "    new_text += min(words, key=lambda w: edit_distance(w, word)) + \" \"\n",
    "new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Разбейте текст из формулировки задания 1 на слова; проведите стемминг и лемматизацию слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     слово     |   стемминг    | лемматизация  \n",
      "       с       |       с       |       с       \n",
      "  велечайшим   |   велечайш    |  велечайший   \n",
      "    усилием    |     усил      |    усилие     \n",
      "  выбравшись   |     выбра     |   выбраться   \n",
      "      из       |      из       |      из       \n",
      "    потока     |     поток     |     поток     \n",
      "   убегающих   |     убега     |    убегать    \n",
      "     людей     |      люд      |    человек    \n",
      "    Кутузов    |     кутуз     |    кутузов    \n",
      "      со       |      со       |       с       \n",
      "    свитой     |     свит      |     свита     \n",
      " уменьшевшейся |   уменьшевш   | уменьшевшийся \n",
      "     вдвое     |     вдво      |     вдвое     \n",
      "    поехал     |     поеха     |    поехать    \n",
      "      на       |      на       |      на       \n",
      "     звуки     |     звук      |     звук      \n",
      "   выстрелов   |    выстрел    |    выстрел    \n",
      "    русских    |     русск     |    русский    \n",
      "    орудий     |     оруд      |    орудие     \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "print(f\"{'слово':^15}|{'стемминг':^15}|{'лемматизация':^15}\")\n",
    "stemmer_ru = SnowballStemmer('russian')\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "for word in word_tokenize(text):\n",
    "    print(f\"{word:^15}|{stemmer_ru.stem(word):^15}|{morph.parse(word)[0].normalized.word:^15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Преобразуйте предложения из формулировки задания 1 в векторы при помощи `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 2, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = '''Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. '''\n",
    "sents = sent_tokenize(text)\n",
    "cv = CountVectorizer()\n",
    "cv.fit(sents)\n",
    "sents_cv = cv.transform(sents).toarray()\n",
    "sents_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'считайте': 32,\n",
       " 'слова': 24,\n",
       " 'из': 12,\n",
       " 'файла': 33,\n",
       " 'litw': 0,\n",
       " 'win': 2,\n",
       " 'txt': 1,\n",
       " 'запишите': 11,\n",
       " 'их': 14,\n",
       " 'список': 31,\n",
       " 'words': 3,\n",
       " 'заданном': 9,\n",
       " 'предложении': 22,\n",
       " 'исправьте': 13,\n",
       " 'все': 5,\n",
       " 'опечатки': 21,\n",
       " 'заменив': 10,\n",
       " 'опечатками': 20,\n",
       " 'на': 16,\n",
       " 'ближайшие': 4,\n",
       " 'смысле': 27,\n",
       " 'расстояния': 23,\n",
       " 'левенштейна': 15,\n",
       " 'ним': 18,\n",
       " 'списка': 29,\n",
       " 'что': 34,\n",
       " 'слове': 25,\n",
       " 'есть': 8,\n",
       " 'опечатка': 19,\n",
       " 'если': 7,\n",
       " 'данное': 6,\n",
       " 'слово': 26,\n",
       " 'не': 17,\n",
       " 'содержится': 28,\n",
       " 'списке': 30}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>george s at the cove  black bean soup</td>\n",
       "      <td>an original recipe created by chef scott meska...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>healthy for them  yogurt popsicles</td>\n",
       "      <td>my children and their friends ask for my homem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>i can t believe it s spinach</td>\n",
       "      <td>these were so go it surprised even me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>italian  gut busters</td>\n",
       "      <td>my sisterinlaw made these for us at a family g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>love is in the air  beef fondue   sauces</td>\n",
       "      <td>i think a fondue is a very romantic casual din...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29995</td>\n",
       "      <td>29995</td>\n",
       "      <td>zurie s holey rustic olive and cheddar bread</td>\n",
       "      <td>this is based on a french recipe but i changed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29996</td>\n",
       "      <td>29996</td>\n",
       "      <td>zwetschgenkuchen  bavarian plum cake</td>\n",
       "      <td>this is a traditional fresh plum cake thought ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29997</td>\n",
       "      <td>29997</td>\n",
       "      <td>zwiebelkuchen   southwest german onion cake</td>\n",
       "      <td>this is a traditional late summer early fall s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29998</td>\n",
       "      <td>29998</td>\n",
       "      <td>zydeco soup</td>\n",
       "      <td>this is a delicious soup that i originally fou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29999</td>\n",
       "      <td>29999</td>\n",
       "      <td>cookies by design   cookies on a stick</td>\n",
       "      <td>ive heard of the cookies by design company but...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                          name  \\\n",
       "0               0         george s at the cove  black bean soup   \n",
       "1               1            healthy for them  yogurt popsicles   \n",
       "2               2                  i can t believe it s spinach   \n",
       "3               3                          italian  gut busters   \n",
       "4               4      love is in the air  beef fondue   sauces   \n",
       "...           ...                                           ...   \n",
       "29995       29995  zurie s holey rustic olive and cheddar bread   \n",
       "29996       29996          zwetschgenkuchen  bavarian plum cake   \n",
       "29997       29997   zwiebelkuchen   southwest german onion cake   \n",
       "29998       29998                                   zydeco soup   \n",
       "29999       29999        cookies by design   cookies on a stick   \n",
       "\n",
       "                                             description  \n",
       "0      an original recipe created by chef scott meska...  \n",
       "1      my children and their friends ask for my homem...  \n",
       "2                  these were so go it surprised even me  \n",
       "3      my sisterinlaw made these for us at a family g...  \n",
       "4      i think a fondue is a very romantic casual din...  \n",
       "...                                                  ...  \n",
       "29995  this is based on a french recipe but i changed...  \n",
       "29996  this is a traditional fresh plum cake thought ...  \n",
       "29997  this is a traditional late summer early fall s...  \n",
       "29998  this is a delicious soup that i originally fou...  \n",
       "29999  ive heard of the cookies by design company but...  \n",
       "\n",
       "[30000 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "preprocessed_descriptions = pd.read_csv(\"./09_string_2_data/preprocessed_descriptions.csv\")\n",
    "preprocessed_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nouba',\n",
       " 'valentines',\n",
       " 'corsican',\n",
       " 'corneri',\n",
       " 'omni',\n",
       " 'biscotti',\n",
       " 'wontons',\n",
       " 'chexcom',\n",
       " '53333',\n",
       " 'my',\n",
       " 'micromanagement',\n",
       " 'laines',\n",
       " 'textureit',\n",
       " 'febmarch',\n",
       " 'nobrainer',\n",
       " 'moisest',\n",
       " 'nonmayo',\n",
       " 'majorly',\n",
       " '457g',\n",
       " 'devil',\n",
       " 'heaps',\n",
       " 'shakersteamer',\n",
       " 'wwwdelmontecom',\n",
       " 'raffia',\n",
       " 'recipeadded',\n",
       " 'nonvegetarian',\n",
       " 'malnati',\n",
       " 'foundation',\n",
       " 'gingerteriyaki',\n",
       " 'faulted',\n",
       " 'hollywoods',\n",
       " 'seattlepost',\n",
       " 'simmerman',\n",
       " 'daughter',\n",
       " 'disguise',\n",
       " 'partygathering',\n",
       " 'dialysis',\n",
       " 'bribe',\n",
       " 'makeamix',\n",
       " 'lands',\n",
       " 'treacle',\n",
       " 'unecessary',\n",
       " 'screen',\n",
       " 'raspberries',\n",
       " 'scallop',\n",
       " 'squared',\n",
       " 'lambrusco',\n",
       " 'potatopeanut',\n",
       " 'beakfast',\n",
       " 'originates',\n",
       " 'onions',\n",
       " 'costco',\n",
       " 'viennayou',\n",
       " 'colcannon',\n",
       " 'fig',\n",
       " 'soho',\n",
       " 'hung',\n",
       " 'aluminumfree',\n",
       " 'thailand',\n",
       " 'heston',\n",
       " 'improves',\n",
       " 'emerilgood',\n",
       " 'appearance',\n",
       " 'hanukkah',\n",
       " 'cutter',\n",
       " 'faucet',\n",
       " '4hmy',\n",
       " 'cooker',\n",
       " 'laa',\n",
       " 'squashes',\n",
       " 'holding',\n",
       " 'chestnutpicking',\n",
       " 'mustardglazed',\n",
       " 'recepie',\n",
       " 'evoking',\n",
       " 'dente',\n",
       " 'dgss',\n",
       " 'spec',\n",
       " 'dans',\n",
       " 'holst',\n",
       " 'chanterelles',\n",
       " 'exactally',\n",
       " 'jalapenosor',\n",
       " 'migrant',\n",
       " 'fishshellfishtonight',\n",
       " 'olive',\n",
       " 'promotional',\n",
       " 'honorable',\n",
       " 'nods',\n",
       " 'gauranteed',\n",
       " 'suitable',\n",
       " 'refered',\n",
       " 'sacrifices',\n",
       " 'lowercalorie',\n",
       " 'lauderdale',\n",
       " 'additionssubsitutions',\n",
       " 'giannatempo',\n",
       " 'toplooks',\n",
       " 'centre',\n",
       " 'hassan',\n",
       " 'swoons',\n",
       " 'pour',\n",
       " 'arizonaprep',\n",
       " 'up',\n",
       " 'poutine',\n",
       " 'demonstrated',\n",
       " 'algerian',\n",
       " 'deborah',\n",
       " 'wwwweightwatcherscom',\n",
       " 'brewpub',\n",
       " 'grams',\n",
       " 'grandparentscom',\n",
       " 'blanching',\n",
       " 'zondervans',\n",
       " 'uncookedfor',\n",
       " 'datenightin',\n",
       " 'clans',\n",
       " 'cinnamons',\n",
       " 'germanaustrian',\n",
       " 'earthquake',\n",
       " 'fairand',\n",
       " 'httpbitlyauhmhp',\n",
       " '4060',\n",
       " 'cabbagelook',\n",
       " 'foothills',\n",
       " '1952',\n",
       " 'rhurbarb',\n",
       " 'greeted',\n",
       " 'zipper',\n",
       " 'dishwashing',\n",
       " 'attracting',\n",
       " 'joyces',\n",
       " 'exboyfriend',\n",
       " 'gimli',\n",
       " 'sealed',\n",
       " 'wganache',\n",
       " 'italianfoodforevercom',\n",
       " 'backan',\n",
       " 'temperture',\n",
       " 'options',\n",
       " 'marty',\n",
       " 'trusted',\n",
       " 'perishable',\n",
       " 'a9',\n",
       " 'preferenceinstead',\n",
       " 'pumpkinsquash',\n",
       " 'cabinet',\n",
       " 'peachtree',\n",
       " '2011i',\n",
       " 'dry',\n",
       " 'disclaimer',\n",
       " 'lighten',\n",
       " 'traveling',\n",
       " '68336',\n",
       " 'freshly',\n",
       " 'veganaise',\n",
       " 'mug',\n",
       " 'oneandonly',\n",
       " 'pill',\n",
       " 'primavera',\n",
       " 'wholl',\n",
       " 'bierocks',\n",
       " 'soup',\n",
       " 'ziplocs',\n",
       " 'dishanother',\n",
       " 'carrs',\n",
       " 'hundreds',\n",
       " 'wasshe',\n",
       " '102508',\n",
       " 'smile',\n",
       " 'darling',\n",
       " 'important',\n",
       " 'beeter',\n",
       " 'rocks',\n",
       " 'itwhich',\n",
       " 'jackolantern',\n",
       " 'teppan',\n",
       " 'anytime',\n",
       " 'kadey',\n",
       " 'jericho',\n",
       " 'californias',\n",
       " 'mccay',\n",
       " 'pekmezi',\n",
       " 'sonomacalifornia',\n",
       " 'oriental',\n",
       " 'placing',\n",
       " 'inhey',\n",
       " 'kats',\n",
       " 'flatshaped',\n",
       " 'combinationwith',\n",
       " 'author',\n",
       " 'until',\n",
       " 'sponsored',\n",
       " 'lighttry',\n",
       " 'ree',\n",
       " 'splnda',\n",
       " '775',\n",
       " 'whole',\n",
       " 'amerindians',\n",
       " 'doevres',\n",
       " 'andree',\n",
       " 'penzeys',\n",
       " 'kulfi',\n",
       " 'cook',\n",
       " 'itand',\n",
       " 'slides',\n",
       " 'anyone',\n",
       " 'guidry',\n",
       " 'meantime',\n",
       " 'effects',\n",
       " 'sitei',\n",
       " 'brothers',\n",
       " 'collect',\n",
       " 'nikibone',\n",
       " 'wished',\n",
       " 'menus4momscom',\n",
       " 'product',\n",
       " 'sa',\n",
       " 'snugly',\n",
       " 'wellington',\n",
       " 'cacao',\n",
       " 'curdy',\n",
       " 'timei',\n",
       " 'rump',\n",
       " '20ounce',\n",
       " 'outthedoor',\n",
       " 'depaz',\n",
       " 'viva',\n",
       " 'regret',\n",
       " 'treo',\n",
       " 'httpthepioneerwomancomcooking200706brisketbaby',\n",
       " 'granitas',\n",
       " 'xanathan',\n",
       " 'oregano',\n",
       " 'steamykitchen',\n",
       " 'challengeand',\n",
       " 'luminous',\n",
       " '4think',\n",
       " 'bestit',\n",
       " 'smashed',\n",
       " 'periodically',\n",
       " '362382',\n",
       " 'pa',\n",
       " 'zammywhat',\n",
       " 'professional',\n",
       " 'rivals',\n",
       " 'mendare',\n",
       " 'contest',\n",
       " 'changed',\n",
       " 'interests',\n",
       " 'perrin',\n",
       " '2710',\n",
       " 'escape',\n",
       " 'fingerlickin',\n",
       " 'snoopy',\n",
       " 'escoffier',\n",
       " '9inch',\n",
       " 'toll',\n",
       " '1950s1960s',\n",
       " 'skinnyminnie',\n",
       " 'fortyfive',\n",
       " 'marvelously',\n",
       " 'gems',\n",
       " 'toppingif',\n",
       " 'syrp',\n",
       " 'gertrudes',\n",
       " 'tooive',\n",
       " 'cleverly',\n",
       " 'websites',\n",
       " 'latinstyle',\n",
       " 'lite',\n",
       " 'plesase',\n",
       " 'flavori',\n",
       " 'sub',\n",
       " 'mitchell',\n",
       " 'backgrounds',\n",
       " 'boilovers',\n",
       " 'sylvias',\n",
       " 'resource',\n",
       " 'feelin',\n",
       " 'hearty',\n",
       " 'lowcarbers',\n",
       " 'wafting',\n",
       " 'desirethese',\n",
       " 'primary',\n",
       " '129',\n",
       " 'yucca',\n",
       " 'score',\n",
       " 'minna',\n",
       " 'automatically',\n",
       " 'calls',\n",
       " 'knownot',\n",
       " '309199',\n",
       " 'wander',\n",
       " 'kinderlehrers',\n",
       " 'distilling',\n",
       " '90928',\n",
       " 'deicided',\n",
       " 'moghul',\n",
       " 'a175',\n",
       " 'reddish',\n",
       " 'couriers',\n",
       " 'joyful',\n",
       " 'safest',\n",
       " 'hillshire',\n",
       " 'lolly',\n",
       " 'codified',\n",
       " 'darts',\n",
       " 'kabsa',\n",
       " 'ins',\n",
       " 'fromkatie',\n",
       " 'reviewed',\n",
       " 'montral',\n",
       " 'velvety',\n",
       " 'pil',\n",
       " 'uniform',\n",
       " 'mu',\n",
       " 'sec',\n",
       " 'impossibletoresist',\n",
       " 'brag',\n",
       " 'cranberries',\n",
       " 'adjust',\n",
       " 'crusty',\n",
       " 'ibs',\n",
       " 'gave',\n",
       " 'p66',\n",
       " 'central',\n",
       " 'partial',\n",
       " 'torode',\n",
       " 'calder',\n",
       " 'rialto',\n",
       " 'acididic',\n",
       " 'testimonials',\n",
       " 'yours',\n",
       " 'pepins',\n",
       " 'menudo',\n",
       " 'footed',\n",
       " 'ecomical',\n",
       " 'scrumptous',\n",
       " 'tilapiaof',\n",
       " 'variant',\n",
       " 'chilpancingo',\n",
       " '253389',\n",
       " 'agreethese',\n",
       " 'recognize',\n",
       " 'standalone',\n",
       " 'revueltos',\n",
       " 'iready',\n",
       " 'onethird',\n",
       " 'litally',\n",
       " 'shower',\n",
       " '14kg',\n",
       " 'pot',\n",
       " 'scarce',\n",
       " 'grates',\n",
       " 'gab',\n",
       " 'fro',\n",
       " 'ketchup',\n",
       " 'alfalfa',\n",
       " 'mizrahi',\n",
       " 'present',\n",
       " 'frustrating',\n",
       " 'httpwwwinsanitytheorynet',\n",
       " 'nurse',\n",
       " 'divided',\n",
       " 'cantalope',\n",
       " 'dennys',\n",
       " 'southernlivingcom',\n",
       " 'packed',\n",
       " 'httpwwwcentercutcookcomchocolatelasagna',\n",
       " 'kasma',\n",
       " 'menthe',\n",
       " 'bark',\n",
       " 'macnab',\n",
       " 'lucinda',\n",
       " 'pictured',\n",
       " 'depression',\n",
       " 'nostalgia',\n",
       " 'sheds',\n",
       " 'saved',\n",
       " 'deceived',\n",
       " 'snackand',\n",
       " 'home',\n",
       " 'antioxidants',\n",
       " 'huei',\n",
       " 'mckee',\n",
       " 'accurate',\n",
       " 'wonderul',\n",
       " 'personalization',\n",
       " '17g',\n",
       " 'lordy',\n",
       " 'tangible',\n",
       " 'itwhenever',\n",
       " 'fideo',\n",
       " 'orangemake',\n",
       " 'massive',\n",
       " 'scovil',\n",
       " 'julias',\n",
       " 'jasmine',\n",
       " 'annacoyne',\n",
       " 'hicks',\n",
       " 'ail',\n",
       " 'didi',\n",
       " 'rockies',\n",
       " 'soba',\n",
       " 'boniatosweet',\n",
       " 'sandwhich',\n",
       " 'scientific',\n",
       " 'phones',\n",
       " 'morish',\n",
       " 'nigel',\n",
       " '911',\n",
       " 'schatz',\n",
       " 'personlized',\n",
       " 'wingsdip',\n",
       " 'whisky',\n",
       " 'nasty',\n",
       " 'halfed',\n",
       " 'fiveinch',\n",
       " '6144',\n",
       " 'oscar',\n",
       " 'fib',\n",
       " 'spain',\n",
       " 'instructor',\n",
       " 'tend',\n",
       " 'heartyfilling',\n",
       " 'stewartmy',\n",
       " 'memories',\n",
       " 'manitoba',\n",
       " '387mg',\n",
       " 'invest',\n",
       " 'vivas',\n",
       " 'icy',\n",
       " 'ancestry',\n",
       " 'concerning',\n",
       " 'seeds',\n",
       " 'sconelike',\n",
       " 'float',\n",
       " 'disasterous',\n",
       " 'wiped',\n",
       " 'bolillos',\n",
       " 'ruckus',\n",
       " 'prefers',\n",
       " 'broth',\n",
       " 'pickerel',\n",
       " '4860',\n",
       " 'ver',\n",
       " 'maniac',\n",
       " 'bundts',\n",
       " 'storage',\n",
       " 'juliano',\n",
       " 'lasagne',\n",
       " 'decandant',\n",
       " 'smelled',\n",
       " 'middletown',\n",
       " 'frozen',\n",
       " 'lora',\n",
       " 'mayoit',\n",
       " 'leanest',\n",
       " 'renunion',\n",
       " 'traditionalist',\n",
       " 'emmantaler',\n",
       " 'wile',\n",
       " 'euchee',\n",
       " '04g',\n",
       " 'purviance',\n",
       " 'taralli',\n",
       " '360192',\n",
       " 'fatherinlaws',\n",
       " 'watermans',\n",
       " 'checking',\n",
       " 'afternoonhigh',\n",
       " 'acrossin',\n",
       " 'sahaha',\n",
       " 'stable',\n",
       " 'resipes',\n",
       " 'frizells',\n",
       " 'publixs',\n",
       " 'claws',\n",
       " 'headbetter',\n",
       " 'tapioca',\n",
       " 'sunflower',\n",
       " 'lazy',\n",
       " 'dilute',\n",
       " 'mcmurl',\n",
       " 'siteonly',\n",
       " 'carefulthey',\n",
       " 'infact',\n",
       " 'arrieta',\n",
       " 'donnellans',\n",
       " 'foldovers',\n",
       " 'anjali',\n",
       " 'danes',\n",
       " 'unbelievable',\n",
       " 'jacobi',\n",
       " 'mixes',\n",
       " 'dogeared',\n",
       " 'exposed',\n",
       " 'hype',\n",
       " 'ingredientswe',\n",
       " 'suffered',\n",
       " 'friedso',\n",
       " '96141',\n",
       " 'flecks',\n",
       " 'leakage',\n",
       " 'veggiesyour',\n",
       " 'shredder',\n",
       " 'fondly',\n",
       " 'waistlines',\n",
       " 'hehehe',\n",
       " 'desssert',\n",
       " 'synergistically',\n",
       " 'offeringcolorful',\n",
       " 'freedom',\n",
       " 'maintained',\n",
       " 'oliver',\n",
       " 'tibetan',\n",
       " 'scald',\n",
       " 'takeone',\n",
       " 'chlorophyll',\n",
       " 'boatloads',\n",
       " 'roquefort',\n",
       " 'guerrero',\n",
       " 'barbecuing',\n",
       " 'flavornice',\n",
       " 'pe',\n",
       " 'meredith',\n",
       " 'manufacture',\n",
       " 'operate',\n",
       " 'meatyou',\n",
       " 'cutty',\n",
       " 'slavonia',\n",
       " 'rattray',\n",
       " 'nicknames',\n",
       " 'plums',\n",
       " 'cavatini',\n",
       " 'gazette',\n",
       " '113',\n",
       " 'flowed',\n",
       " 'shop',\n",
       " 'heckuva',\n",
       " 'condition',\n",
       " 'flood',\n",
       " '2440',\n",
       " 'yang',\n",
       " 'literal',\n",
       " 'conical',\n",
       " 'bottling',\n",
       " 'investigation',\n",
       " 'liquer',\n",
       " 'gaucho',\n",
       " 'communion',\n",
       " 'disneyland',\n",
       " 'jolly',\n",
       " 'aperitiefs',\n",
       " 'breakfasts',\n",
       " 'blanc',\n",
       " 'diable',\n",
       " 'toasting',\n",
       " 'johns',\n",
       " 'aroundthese',\n",
       " 'electic',\n",
       " 'empty',\n",
       " 'httpbitly5skaeh',\n",
       " 'slight',\n",
       " 'tigers',\n",
       " 'floridanative',\n",
       " 'bewaremake',\n",
       " 'voeller',\n",
       " 'moma',\n",
       " 'easy',\n",
       " 'thicksliced',\n",
       " '06mg',\n",
       " 'yee',\n",
       " 'nantucket',\n",
       " 'moneymaking',\n",
       " 'breadbowl',\n",
       " 'time',\n",
       " 'animals',\n",
       " 'summer',\n",
       " 'chiang',\n",
       " 'hides',\n",
       " 'bratten',\n",
       " 'timess',\n",
       " 'alanleonettiqcom',\n",
       " 'foie',\n",
       " 'skiing',\n",
       " 'hassled',\n",
       " 'chervils',\n",
       " 'stawberry',\n",
       " 'cheapest',\n",
       " 'will',\n",
       " '324282',\n",
       " 'mellow',\n",
       " 'daubes',\n",
       " 'bologna',\n",
       " 'donnalee',\n",
       " 'usd',\n",
       " '2year',\n",
       " 'stacys',\n",
       " 'thornberryit',\n",
       " 'prework',\n",
       " 'omega',\n",
       " 'boureki',\n",
       " 'mozzarella',\n",
       " 'honduran',\n",
       " '9ths',\n",
       " 'deliciousmy',\n",
       " 'shockey',\n",
       " '13cup',\n",
       " 'httpwwwrecipezaarcom63785',\n",
       " 'novelty',\n",
       " 'thank',\n",
       " 'dbil',\n",
       " 'breadall',\n",
       " 'toni',\n",
       " 'benefiber',\n",
       " 'relatively',\n",
       " '464144',\n",
       " 'moxies',\n",
       " 'cheerioscom',\n",
       " '78579',\n",
       " '8th',\n",
       " 'telephone',\n",
       " 'boned',\n",
       " 'grainsweetened',\n",
       " 'celbrate',\n",
       " 'wheat',\n",
       " 'perdue',\n",
       " 'shabbat',\n",
       " 'bakeoff',\n",
       " 'vol',\n",
       " 'risoni',\n",
       " 'verybestbakingcom',\n",
       " 'stovetop',\n",
       " 'hebrew',\n",
       " 'along',\n",
       " 'saladif',\n",
       " 'calvados',\n",
       " 'springyness',\n",
       " 'gingercuminscented',\n",
       " 'potlucksits',\n",
       " 'elmers',\n",
       " 'alaskan',\n",
       " 'kimkes',\n",
       " 'classics',\n",
       " 'nofuss',\n",
       " 'themmake',\n",
       " 'somewherethink',\n",
       " 'soundspromise',\n",
       " 'addomit',\n",
       " 'itemized',\n",
       " 'must',\n",
       " 'fettuccine',\n",
       " 'poshest',\n",
       " 'httpwwwyoutubecomwatchvtktymx2uiq',\n",
       " 'lessen',\n",
       " 'youngsters',\n",
       " 'roastloaf',\n",
       " 'devonshire',\n",
       " 'handle',\n",
       " 'apportioned',\n",
       " 'linkhttpwwwgreekacomcycladessantorinirecipesskordomakaronahtm',\n",
       " 'assorted',\n",
       " 'iceberg',\n",
       " 'mindy',\n",
       " 'reheated',\n",
       " 'webby',\n",
       " 'fn',\n",
       " 'wales',\n",
       " 'cincin',\n",
       " 'recipehope',\n",
       " 'dorowat',\n",
       " '182515',\n",
       " 'humility',\n",
       " '148502',\n",
       " 'kiawe',\n",
       " 'refuel',\n",
       " 'increments',\n",
       " 'royer',\n",
       " 'auxiliary',\n",
       " 'hardtack',\n",
       " 'butive',\n",
       " 'crowberries',\n",
       " 'onefifth',\n",
       " 'incudes',\n",
       " 'nutsprobably',\n",
       " 'vietnamesethai',\n",
       " 'tg',\n",
       " 'etymology',\n",
       " 'syringe',\n",
       " 'healtheditdepending',\n",
       " 'head',\n",
       " 'flora',\n",
       " '40c',\n",
       " '15g',\n",
       " 'enamoured',\n",
       " 'devastated',\n",
       " 'covers',\n",
       " 'oktoberfest',\n",
       " 'pat',\n",
       " 'removed',\n",
       " 'laughter',\n",
       " 'kfc',\n",
       " 'east',\n",
       " 'chickensour',\n",
       " 'marinadebrine',\n",
       " 'roosevelt',\n",
       " '8the',\n",
       " 'homecoming',\n",
       " 'bouillie',\n",
       " 'harbor',\n",
       " 'stocks',\n",
       " 'judy',\n",
       " 'podelski',\n",
       " 'lily',\n",
       " 'scandinavia',\n",
       " 'powedered',\n",
       " '24175',\n",
       " 'poppycock',\n",
       " 'yeas',\n",
       " 'geordie',\n",
       " 'littlei',\n",
       " 'flavorsthe',\n",
       " 'hbo',\n",
       " 'vietnameseinspired',\n",
       " 'dished',\n",
       " 'otheri',\n",
       " 'gives',\n",
       " 'flooded',\n",
       " 'vincent',\n",
       " 'whoa',\n",
       " '411',\n",
       " 'grogan',\n",
       " 'slowbarbecued',\n",
       " 'delicious',\n",
       " 'oporto',\n",
       " 'zakuski',\n",
       " 'feel',\n",
       " 'httptinyurlcomd7b834',\n",
       " 'hassle',\n",
       " 'desiree',\n",
       " 'lennon',\n",
       " 'easier',\n",
       " 'kline',\n",
       " 'sprinkle',\n",
       " '91',\n",
       " 'relieve',\n",
       " 'campground',\n",
       " 'overheated',\n",
       " 'carottes',\n",
       " 'responders',\n",
       " 'wife',\n",
       " 'bettycrockercom',\n",
       " 'selling',\n",
       " 'spray',\n",
       " 'misadventures',\n",
       " 'nomnoms',\n",
       " 'soso',\n",
       " 'httpwwwrealagecomnutritioncenterrecipesportobellogravyaspx',\n",
       " 'saladsandwich',\n",
       " 'wingsenough',\n",
       " 'bbqng',\n",
       " 'smelling',\n",
       " 'breadbaguette',\n",
       " 'sageparmesan',\n",
       " 'jalapeos',\n",
       " 'scooter',\n",
       " 'simplify',\n",
       " 'hogwarts',\n",
       " 'purplehullcom',\n",
       " 'sulphur',\n",
       " 'digitalcookbooktv',\n",
       " '184187',\n",
       " 'oz',\n",
       " 'hollowed',\n",
       " 'halmonie',\n",
       " 'made',\n",
       " 'varenyky',\n",
       " 'buffetsit',\n",
       " 'mcferran',\n",
       " 'prefrozen',\n",
       " 'chatni',\n",
       " 'burnet',\n",
       " 'rooftop',\n",
       " 'nans',\n",
       " 'chelow',\n",
       " '162586',\n",
       " 'fixit',\n",
       " 'mkennon',\n",
       " 'maca',\n",
       " 'suretobetasty',\n",
       " 'frangelico',\n",
       " 'clam',\n",
       " 'allstar',\n",
       " 'fishi',\n",
       " 'houlihans',\n",
       " 'pointy',\n",
       " 'rachaels',\n",
       " 'cannister',\n",
       " 'ast',\n",
       " 'typo',\n",
       " 'cakeenjoyit',\n",
       " 'forgetuntil',\n",
       " 'fudgy',\n",
       " 'bharta',\n",
       " '83226',\n",
       " 'greatlooking',\n",
       " 'writerchef',\n",
       " 'oneand',\n",
       " 'scotlands',\n",
       " 'knuckles',\n",
       " 'inexperienced',\n",
       " 'urbanachampaign',\n",
       " 'admired',\n",
       " '460yearold',\n",
       " 'gardenswith',\n",
       " 'breakfastsubmitted',\n",
       " 'pattypan',\n",
       " 'saulsbury',\n",
       " 'greggs',\n",
       " 'surname',\n",
       " 'schools',\n",
       " 'souffles',\n",
       " 'ones',\n",
       " 'create',\n",
       " 'latex',\n",
       " 'barbecuecookout',\n",
       " 'belongings',\n",
       " 'carrot',\n",
       " '10sec',\n",
       " 'kittys',\n",
       " 'chaitea',\n",
       " 'shingle',\n",
       " 'frenchstyle',\n",
       " 'awayand',\n",
       " 'sept1997',\n",
       " 'disregard',\n",
       " 'seasoningenjoy',\n",
       " 'durring',\n",
       " 'nights',\n",
       " 'sunken',\n",
       " 'refridge',\n",
       " '159mg',\n",
       " 'monkfishit',\n",
       " 'melted',\n",
       " 'foleys',\n",
       " 'dishquick',\n",
       " 'asap',\n",
       " 'unseasoned',\n",
       " 'recieve',\n",
       " 'doeuvre',\n",
       " 'yummie',\n",
       " 'kamut',\n",
       " 'lasagnalike',\n",
       " 'unctuousness',\n",
       " 'immersionstick',\n",
       " 'guinnessy',\n",
       " 'rellenos',\n",
       " 'newly',\n",
       " 'veganized',\n",
       " 'vegcookingcom',\n",
       " 'wwwinmamaskitchencom',\n",
       " 'marques',\n",
       " 'au',\n",
       " 'bobby',\n",
       " 'directory',\n",
       " 'sponginess',\n",
       " 'contributes',\n",
       " 'braises',\n",
       " 'aahs',\n",
       " 'birthdayit',\n",
       " 'crisp',\n",
       " 'replica',\n",
       " 'poppy',\n",
       " 'gardiner',\n",
       " 'lookinggreat',\n",
       " '1780',\n",
       " 'benifits',\n",
       " 'stockings',\n",
       " 'tesco',\n",
       " '2ingredient',\n",
       " 'wqe',\n",
       " 'eateries',\n",
       " 'goudas',\n",
       " 'sonomas',\n",
       " 'government',\n",
       " 'vellody',\n",
       " 'potentially',\n",
       " 'mediterrran',\n",
       " 'japan',\n",
       " 'dee514',\n",
       " 'germanbenelux',\n",
       " 'comments',\n",
       " 'slaws',\n",
       " 'cavaiani',\n",
       " 'cantaloup',\n",
       " 'youre',\n",
       " 'luau',\n",
       " 'scotch',\n",
       " 'curls',\n",
       " 'heheheit',\n",
       " 'stovvetop',\n",
       " 'herbsthen',\n",
       " 'candian',\n",
       " 'lesser',\n",
       " 'recipeits',\n",
       " 'dumped',\n",
       " 'confirm',\n",
       " 'trace',\n",
       " 'easyand',\n",
       " 'nextdoor',\n",
       " 'receive',\n",
       " 'heats',\n",
       " 'basilmint',\n",
       " 'shakamak',\n",
       " 'dressing',\n",
       " 'appeal',\n",
       " 'preheat',\n",
       " 'preserved',\n",
       " 'quickbake',\n",
       " 'strackbein',\n",
       " 'decedent',\n",
       " 'stirins',\n",
       " 'arrived',\n",
       " 'cookbookvery',\n",
       " 'dancing',\n",
       " 'gibsons',\n",
       " 'relishes',\n",
       " 'suffer',\n",
       " 'redhot',\n",
       " '41126',\n",
       " 'share',\n",
       " 'rear',\n",
       " 'procssor',\n",
       " 'admitted',\n",
       " 'endocrinologists',\n",
       " 'oaxacas',\n",
       " 'well',\n",
       " 'tenderfalls',\n",
       " 'endora',\n",
       " 'gently',\n",
       " 'utmost',\n",
       " 'chutzpah',\n",
       " 'sliders',\n",
       " 'cake',\n",
       " 'designate',\n",
       " 'sinful',\n",
       " 'mestizajemixture',\n",
       " 'itlol',\n",
       " 'surface',\n",
       " 'goodthese',\n",
       " 'salmonrad',\n",
       " 'nearly',\n",
       " 'mmmmhavent',\n",
       " 'is',\n",
       " 'manual',\n",
       " 'sandwicha',\n",
       " 'reiner',\n",
       " 'httpwwwrecipezaarcomcookbookphpbookid150466',\n",
       " 'northwood',\n",
       " 'cleary',\n",
       " 'avocado',\n",
       " 'spurofthemoment',\n",
       " 'monster',\n",
       " 'leafiness',\n",
       " 'summerwe',\n",
       " '43',\n",
       " 'deserve',\n",
       " 'took',\n",
       " 'coworkers',\n",
       " 'guiltfree',\n",
       " 'pebbled',\n",
       " 'stray',\n",
       " 'striped',\n",
       " 'sickly',\n",
       " 'exmotherinlaw',\n",
       " 'leeds',\n",
       " 'appitizers',\n",
       " 'peoples',\n",
       " '139883',\n",
       " 'chef231057',\n",
       " 'docks',\n",
       " 'batman',\n",
       " 'traditionalstyle',\n",
       " '110',\n",
       " 'grms',\n",
       " 'shrimpin',\n",
       " 'quicheomelet',\n",
       " 'leetle',\n",
       " 'ita',\n",
       " 'cand',\n",
       " 'inlaw',\n",
       " 'danze',\n",
       " 'absorbed',\n",
       " 'attempts',\n",
       " 'taylors',\n",
       " 'originals',\n",
       " 'insides',\n",
       " 'washington',\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for desc in preprocessed_descriptions[\"description\"]:\n",
    "    words.extend(word_tokenize(str(desc)))\n",
    "words = list(set(words))\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95600 - subjective - 10 - 15 - 15\n",
      "yearsnot - hannahs - 7 - 11 - 11\n",
      "gazette - herebut - 6 - 10 - 10\n",
      "tastesdiet - rousso - 9 - 12 - 12\n",
      "basilspinach - drable - 11 - 14 - 13\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "pairs = [sample(words, 2) for _ in range(5)]\n",
    "for pair in pairs:\n",
    "    print(f\"{pair[0]} - {pair[1]} - {edit_distance(pair[0], pair[1])} - {edit_distance(pair[0], pair[1], substitution_cost=2)} - {edit_distance(pair[0], pair[1], substitution_cost=2, transpositions=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "copy_words = copy.copy(words)\n",
    "\n",
    "def finder(word, k):\n",
    "    similar_words = []\n",
    "    for i in range(k):\n",
    "        similar_words.append(min(copy_words, key=lambda w: edit_distance(w, word)))\n",
    "        copy_words.remove(similar_words[i])\n",
    "        \n",
    "finder(\"tasted\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: \n",
    "    * word\n",
    "    * stemmed_word \n",
    "    * normalized_word \n",
    "\n",
    "Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "normalized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "normalized_words2 = [morph.parse(word)[0].normalized.word for word in words]\n",
    "\n",
    "stemmed_normalized_words = pd.DataFrame({\n",
    "    'word': words, \n",
    "    'stemmed_word': stemmed_words, \n",
    "    'normalized_word': normalized_words,\n",
    "    'normalized_word2': normalized_words2}).set_index('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_word</th>\n",
       "      <th>normalized_word</th>\n",
       "      <th>normalized_word2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>shampoo</td>\n",
       "      <td>shampoo</td>\n",
       "      <td>shampoo</td>\n",
       "      <td>shampoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>samosa</td>\n",
       "      <td>samosa</td>\n",
       "      <td>samosa</td>\n",
       "      <td>samosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>creambut</td>\n",
       "      <td>creambut</td>\n",
       "      <td>creambut</td>\n",
       "      <td>creambut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>reviewer</td>\n",
       "      <td>review</td>\n",
       "      <td>reviewer</td>\n",
       "      <td>reviewer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>rolled</td>\n",
       "      <td>roll</td>\n",
       "      <td>rolled</td>\n",
       "      <td>rolled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ceps</td>\n",
       "      <td>cep</td>\n",
       "      <td>ceps</td>\n",
       "      <td>ceps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bitesized</td>\n",
       "      <td>bites</td>\n",
       "      <td>bitesized</td>\n",
       "      <td>bitesized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>simpletakes</td>\n",
       "      <td>simpletak</td>\n",
       "      <td>simpletakes</td>\n",
       "      <td>simpletakes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>presence</td>\n",
       "      <td>presenc</td>\n",
       "      <td>presence</td>\n",
       "      <td>presence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>didi</td>\n",
       "      <td>didi</td>\n",
       "      <td>didi</td>\n",
       "      <td>didi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            stemmed_word normalized_word normalized_word2\n",
       "word                                                     \n",
       "shampoo          shampoo         shampoo          shampoo\n",
       "samosa            samosa          samosa           samosa\n",
       "creambut        creambut        creambut         creambut\n",
       "reviewer          review        reviewer         reviewer\n",
       "rolled              roll          rolled           rolled\n",
       "ceps                 cep            ceps             ceps\n",
       "bitesized          bites       bitesized        bitesized\n",
       "simpletakes    simpletak     simpletakes      simpletakes\n",
       "presence         presenc        presence         presence\n",
       "didi                didi            didi             didi"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_normalized_words.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>description_without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>george s at the cove  black bean soup</td>\n",
       "      <td>an original recipe created by chef scott meska...</td>\n",
       "      <td>original recipe created chef scott meskan geor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>healthy for them  yogurt popsicles</td>\n",
       "      <td>my children and their friends ask for my homem...</td>\n",
       "      <td>children friends ask homemade popsicles mornin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>i can t believe it s spinach</td>\n",
       "      <td>these were so go it surprised even me</td>\n",
       "      <td>go surprised even</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>italian  gut busters</td>\n",
       "      <td>my sisterinlaw made these for us at a family g...</td>\n",
       "      <td>sisterinlaw made us family get together delici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>love is in the air  beef fondue   sauces</td>\n",
       "      <td>i think a fondue is a very romantic casual din...</td>\n",
       "      <td>think fondue romantic casual dinner wonderful ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29995</td>\n",
       "      <td>29995</td>\n",
       "      <td>zurie s holey rustic olive and cheddar bread</td>\n",
       "      <td>this is based on a french recipe but i changed...</td>\n",
       "      <td>based french recipe changed substantially warn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29996</td>\n",
       "      <td>29996</td>\n",
       "      <td>zwetschgenkuchen  bavarian plum cake</td>\n",
       "      <td>this is a traditional fresh plum cake thought ...</td>\n",
       "      <td>traditional fresh plum cake thought originated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29997</td>\n",
       "      <td>29997</td>\n",
       "      <td>zwiebelkuchen   southwest german onion cake</td>\n",
       "      <td>this is a traditional late summer early fall s...</td>\n",
       "      <td>traditional late summer early fall snack usual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29998</td>\n",
       "      <td>29998</td>\n",
       "      <td>zydeco soup</td>\n",
       "      <td>this is a delicious soup that i originally fou...</td>\n",
       "      <td>delicious soup originally found better homes g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29999</td>\n",
       "      <td>29999</td>\n",
       "      <td>cookies by design   cookies on a stick</td>\n",
       "      <td>ive heard of the cookies by design company but...</td>\n",
       "      <td>ive heard cookies design company never tried c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                          name  \\\n",
       "0               0         george s at the cove  black bean soup   \n",
       "1               1            healthy for them  yogurt popsicles   \n",
       "2               2                  i can t believe it s spinach   \n",
       "3               3                          italian  gut busters   \n",
       "4               4      love is in the air  beef fondue   sauces   \n",
       "...           ...                                           ...   \n",
       "29995       29995  zurie s holey rustic olive and cheddar bread   \n",
       "29996       29996          zwetschgenkuchen  bavarian plum cake   \n",
       "29997       29997   zwiebelkuchen   southwest german onion cake   \n",
       "29998       29998                                   zydeco soup   \n",
       "29999       29999        cookies by design   cookies on a stick   \n",
       "\n",
       "                                             description  \\\n",
       "0      an original recipe created by chef scott meska...   \n",
       "1      my children and their friends ask for my homem...   \n",
       "2                  these were so go it surprised even me   \n",
       "3      my sisterinlaw made these for us at a family g...   \n",
       "4      i think a fondue is a very romantic casual din...   \n",
       "...                                                  ...   \n",
       "29995  this is based on a french recipe but i changed...   \n",
       "29996  this is a traditional fresh plum cake thought ...   \n",
       "29997  this is a traditional late summer early fall s...   \n",
       "29998  this is a delicious soup that i originally fou...   \n",
       "29999  ive heard of the cookies by design company but...   \n",
       "\n",
       "                           description_without_stopwords  \n",
       "0      original recipe created chef scott meskan geor...  \n",
       "1      children friends ask homemade popsicles mornin...  \n",
       "2                                      go surprised even  \n",
       "3      sisterinlaw made us family get together delici...  \n",
       "4      think fondue romantic casual dinner wonderful ...  \n",
       "...                                                  ...  \n",
       "29995  based french recipe changed substantially warn...  \n",
       "29996  traditional fresh plum cake thought originated...  \n",
       "29997  traditional late summer early fall snack usual...  \n",
       "29998  delicious soup originally found better homes g...  \n",
       "29999  ive heard cookies design company never tried c...  \n",
       "\n",
       "[30000 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "description_without_stopwords = []\n",
    "for desc in preprocessed_descriptions[\"description\"]:\n",
    "    notstopwords = []\n",
    "    for word in word_tokenize(str(desc)):\n",
    "        if word not in stopwords:\n",
    "            notstopwords.append(word)\n",
    "    description_without_stopwords.append(\" \".join(notstopwords)) \n",
    "preprocessed_descriptions['description_without_stopwords'] = description_without_stopwords\n",
    "preprocessed_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 40210), ('a', 34994), ('and', 30279), ('this', 27048), ('i', 25111), ('to', 23499), ('is', 20290), ('it', 19863), ('of', 18372), ('for', 15988)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "words = []\n",
    "for desc in preprocessed_descriptions[\"description\"]:\n",
    "    words.extend(word_tokenize(str(desc)))\n",
    "k_with = len(words)\n",
    "\n",
    "fdist = FreqDist(words)\n",
    "print(fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('recipe', 14957), ('make', 6353), ('time', 5180), ('use', 4635), ('great', 4453), ('like', 4175), ('easy', 4175), ('one', 3886), ('good', 3820), ('made', 3814)]\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for desc in description_without_stopwords:\n",
    "        words.extend(word_tokenize(desc))\n",
    "k_without = len(words)\n",
    "\n",
    "fdist = FreqDist(words)\n",
    "print(fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.258384180453824"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100-k_without/k_with*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "{'it': 5, 'isnt': 4, 'hard': 3, 'to': 8, 'make': 6, 'your': 9, 'own': 7, 'cold': 2, 'breakfast': 0, 'cereal': 1}\n",
      "(8, 7)\n",
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]]\n",
      "{'brownie': 1, 'crust': 3, 'and': 0, 'cheesecake': 2, 'need': 5, 'say': 6, 'more': 4}\n",
      "(18, 16)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "{'great': 5, 'dish': 2, 'for': 4, 'those': 13, 'of': 10, 'you': 15, 'who': 14, 'like': 7, 'me': 9, 'love': 8, 'the': 12, 'sharp': 11, 'flavours': 3, 'lemon': 6, 'and': 0, 'capers': 1}\n",
      "(7, 5)\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "{'good': 2, 'start': 4, 'on': 3, 'chilly': 0, 'day': 1}\n",
      "(30, 26)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n",
      "{'using': 23, 'grannys': 13, 'strawberry': 21, 'cake': 3, 'from': 9, 'recipezaar': 19, 'as': 0, 'inspiration': 14, 'created': 6, 'this': 22, 'gf': 12, 'for': 8, 'my': 16, 'celiac': 4, 'daughters': 7, 'birthday': 1, 'frost': 10, 'it': 15, 'with': 25, 'simple': 20, 'vanilla': 24, 'buttercream': 2, 'frosting': 11, 'pink': 18, 'of': 17, 'course': 5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer()\n",
    "for desc in preprocessed_descriptions[\"description\"].sample(5):\n",
    "    corpus_tv = tv.fit_transform(word_tokenize(desc))\n",
    "    print(corpus_tv.shape)\n",
    "    print(corpus_tv.toarray())\n",
    "    print(tv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 149)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.47305674 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.35317941 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.28494293 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.28494293\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.56988585 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.35317941 0.         0.         0.19897512 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.18336394 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.13689768 0.\n",
      "  0.         0.27379536 0.         0.         0.         0.\n",
      "  0.13689768 0.         0.         0.         0.         0.\n",
      "  0.13689768 0.         0.         0.         0.         0.\n",
      "  0.13689768 0.         0.         0.         0.13689768 0.\n",
      "  0.         0.         0.         0.         0.         0.27379536\n",
      "  0.         0.         0.         0.13689768 0.         0.\n",
      "  0.         0.         0.         0.         0.13689768 0.\n",
      "  0.         0.         0.13689768 0.         0.         0.27379536\n",
      "  0.         0.         0.         0.         0.         0.09168197\n",
      "  0.22089637 0.27379536 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.13689768 0.         0.11044819\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.13689768 0.11044819 0.\n",
      "  0.27379536 0.         0.13689768 0.11044819 0.         0.\n",
      "  0.         0.13689768 0.         0.         0.         0.\n",
      "  0.         0.06523248 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.22089637 0.         0.\n",
      "  0.         0.         0.         0.30850306 0.         0.27379536\n",
      "  0.13689768 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.13689768 0.11044819]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.38523454 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.28761224 0.\n",
      "  0.28761224 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.28761224 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.19261727\n",
      "  0.         0.28761224 0.23204374 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.57522448 0.23204374 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.13704877 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.16203572 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.35038823 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.43429718 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.43429718 0.35038823 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.43429718 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.43429718 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n",
      "{'this': 135, 'is': 77, 'wwcooking': 146, 'light': 83, 'recipe': 115, 'these': 132, 'bars': 18, 'are': 16, 'truly': 139, 'delicious': 42, 'you': 148, 'cannot': 32, 'tell': 126, 'that': 128, 'they': 133, 'much': 95, 'lower': 88, 'in': 74, 'fat': 53, 'and': 14, 'calories': 30, 'than': 127, 'most': 94, 'theyre': 134, 'dense': 43, 'full': 63, 'of': 100, 'flavor': 57, 'exchanges': 51, '112': 2, 'starch': 124, '12': 3, 'per': 108, 'serving': 119, '142': 4, '43g': 9, 'saturated': 117, '26g': 8, 'protein': 112, '16g': 5, 'carbohydrate': 33, '24g': 6, 'fiber': 55, '03g': 0, 'cholesterol': 34, '8mg': 10, 'iron': 76, '06mg': 1, 'sodium': 122, '95mg': 11, 'calcium': 28, '24mg': 7, 'courtesy': 38, 'bobby': 21, 'deen': 41, 'kroppkakor': 81, 'eaten': 48, 'all': 12, 'over': 105, 'sweden': 125, 'really': 114, 'oldfashioned': 101, 'cooking': 36, 'the': 129, 'varying': 140, 'different': 44, 'parts': 107, 'it': 78, 'made': 89, 'with': 144, 'only': 103, 'raw': 113, 'potatoes': 109, 'or': 104, 'boiled': 22, 'just': 80, 'as': 17, 'allspice': 13, 'not': 99, 'something': 123, 'everybody': 50, 'likes': 84, 'for': 58, 'me': 91, 'its': 79, 'must': 96, 'though': 136, 'if': 73, 'there': 131, 'any': 15, 'leftovers': 82, 'love': 87, 'to': 138, 'slice': 121, 'them': 130, 'halves': 67, 'next': 97, 'day': 40, 'fry': 61, 'buttermmm': 26, 'friend': 59, 'mine': 93, 'from': 60, 'north': 98, 'does': 46, 'but': 25, 'he': 69, 'makes': 90, 'sauce': 118, 'by': 27, 'pouring': 110, 'milk': 92, 'into': 75, 'frying': 62, 'pan': 106, 'boiling': 23, 'halved': 66, 'want': 141, 'his': 70, 'both': 24, 'cooked': 35, 'one': 102, 'few': 54, 'dishes': 45, 'drink': 47, 'lingonsylt': 85, 'believe': 19, 'called': 29, 'cowberry': 39, 'red': 116, 'whortleberry': 143, 'hope': 71, 'enjoy': 49, 'looked': 86, 'was': 142, 'best': 20, 'could': 37, 'find': 56, 'got': 64, 'hungrybrowsercom': 72, 'prep': 111, 'time': 137, 'guess': 65, 'work': 145, 'since': 120, 'havent': 68, 'yet': 147, 'came': 31, 'experimenting': 52}\n"
     ]
    }
   ],
   "source": [
    "tv = TfidfVectorizer()\n",
    "tv.fit(preprocessed_descriptions[\"description\"].sample(5))\n",
    "result = tv.transform(preprocessed_descriptions[\"description\"].sample(5))\n",
    "print(result.shape)\n",
    "print(result.toarray())\n",
    "print(tv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Вычислите близость между каждой парой рецептов, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Какие рецепты являются наиболее похожими? Прокомментируйте результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
