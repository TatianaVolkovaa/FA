{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqC4R7SGseKa"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J2RM8f5wP33"
      },
      "source": [
        "## 2.1 Создание нейронов и полносвязных слоев"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2ArJn_nsdZC"
      },
      "source": [
        "2.1.1. Используя операции над матрицами и векторами из библиотеки `torch`, реализовать нейрон с заданными весами `weights` и `bias`. Прогнать вектор `inputs` через нейрон и вывести результат. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4agkY9WqPwe"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "  def __init__(self, weights, bias):\n",
        "    # <создать атрибуты объекта weights и bias>\n",
        "    self.weights = weights\n",
        "    self.bias = bias\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    # <реализовать логику нейрона>\n",
        "    return torch.dot(inputs, self.weights).sum() + self.bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJRkSkHHsb7u"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
        "weights = torch.tensor([-0.2, 0.3, -0.5, 0.7])\n",
        "bias = 3.14"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_neuron = Neuron(weights, bias)\n",
        "my_neuron.forward(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyFLEOInM5Gb",
        "outputId": "1e26f8a2-fbdc-49e4-ea3f-782311affe6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.8400)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qJvnwiyty37"
      },
      "source": [
        "2.1.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой с заданными весами `weights` и `biases`. Прогнать вектор `inputs` через слой и вывести результат. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVWF3a9vtx90"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "  def __init__(self, weights, biases):\n",
        "    # <создать атрибуты объекта weights и biases>\n",
        "    self.weights = weights\n",
        "    self.biases = biases    \n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    # <реализовать логику слоя>\n",
        "    return torch.matmul(inputs, self.weights) + self.biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo-JFnHPuFCS"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
        "weights = torch.tensor([[-0.2, 0.3, -0.5, 0.7],\n",
        "                        [0.5, -0.91, 0.26, -0.5],\n",
        "                        [-0.26, -0.27, 0.17, 0.87]]).T\n",
        "biases = torch.tensor([3.14, 2.71, 7.2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.matmul(inputs, weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQYBgsgDK4hc",
        "outputId": "137f9c9a-955a-4709-c747-01b9bf2f07b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.7000, -2.5400,  3.1900])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin_neuron = Linear(weights, biases)\n",
        "lin_neuron.forward(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wah35Xw8Tiad",
        "outputId": "d7076eb0-37bc-470f-cdf2-b87970955290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 4.8400,  0.1700, 10.3900])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQtsJzcxuyGd"
      },
      "source": [
        "2.1.3 Реализовать полносвязный слой из __2.1.2__ таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. Продемонстрировать работу.\n",
        "Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear:\n",
        "  def __init__(self, weights, biases):\n",
        "    # <создать атрибуты объекта weights и biases>\n",
        "    self.weights = weights\n",
        "    self.biases = biases    \n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    # <реализовать логику слоя>\n",
        "    return torch.matmul(inputs, self.weights) + self.biases"
      ],
      "metadata": {
        "id": "wLsTnJrbW_Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8IizmtsuhO1"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
        "                       [2, 5, -1, 2], \n",
        "                       [-1.5, 2.7, 3.3, -0.8]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin_neuron = Linear(weights, biases)\n",
        "lin_neuron.forward(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGzudPLRXF9x",
        "outputId": "48595ee5-ffaf-4071-f80c-ce275b0316fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3.7900,  0.9200,  9.0850],\n",
              "        [ 6.1400, -2.1000,  6.9000],\n",
              "        [ 2.0400,  0.7610,  6.7260]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ2OxH4_vBLu"
      },
      "source": [
        "2.1.4 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой из `n_neurons` нейронов с `n_features` весами у каждого нейрона (инициализируются из стандартного нормального распределения). Прогнать вектор `inputs` через слой и вывести результат. Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOv52EdovASs"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "  def __init__(self, n_features, n_neurons):\n",
        "    # <создать атрибуты объекта weights и biases>\n",
        "    self.weights = torch.randn(n_features, n_neurons)\n",
        "    self.biases = torch.zeros(n_neurons)    \n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    # <реализовать логику слоя>\n",
        "    return torch.matmul(inputs, self.weights) + self.biases"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
        "                       [2, 5, -1, 2], \n",
        "                       [-1.5, 2.7, 3.3, -0.8]])\n",
        "\n",
        "linear = Linear(n_features = 4, n_neurons = 3)\n",
        "linear.forward(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IXbrjMJVApE",
        "outputId": "618c6a34-4c17-43e1-a8cb-60cbd84b4577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-5.6897, -5.2130, -1.7040],\n",
              "        [ 7.0606, -4.3409,  5.2954],\n",
              "        [-4.0935, -7.0265,  4.4285]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPG4UqL4wajI"
      },
      "source": [
        "2.1.5 Используя решение из __2.1.4__, создать 2 полносвязных слоя и пропустить матрицу `inputs` последовательно через эти два слоя. Количество нейронов в первом слое выбрать произвольно, количество нейронов во втором слое выбрать так, чтобы результатом прогона являлась матрица (3x7). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjjQIQlTxJE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf89c2f-8b60-4d8d-8a31-7a47ba2d195e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-4.8435e-01,  4.7903e+00,  4.6006e+00, -5.6316e+00, -2.1731e+00,\n",
              "         -1.3729e+00,  7.7546e+00],\n",
              "        [-5.0718e+00,  1.9435e+00, -1.3836e+01,  6.7717e+00,  6.9530e+00,\n",
              "          1.5972e+01, -3.1531e+00],\n",
              "        [-1.3095e-01, -5.8354e-03, -2.1411e+00,  1.3036e+00,  7.8872e-01,\n",
              "          2.0645e+00, -5.5715e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
        "                       [2, 5, -1, 2], \n",
        "                       [-1.5, 2.7, 3.3, -0.8]])\n",
        "\n",
        "linear_1 = Linear(n_features = 4, n_neurons = 3)\n",
        "linear_2 = Linear(n_features = 3, n_neurons = 7)\n",
        "outs1 = linear_1.forward(inputs)\n",
        "outs2 = linear_2.forward(outs1)\n",
        "outs2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_1.biases, linear_1.weights, outs1, linear_2.biases, linear_2.weights, outs2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7lHkDM3-WTo",
        "outputId": "45ac953d-5c09-48f9-bd31-5bc2b3366382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 0., 0.]), tensor([[-1.9928, -1.4443,  1.1424],\n",
              "         [ 0.0441,  0.2870,  0.5159],\n",
              "         [-0.8369,  0.1171,  0.9405],\n",
              "         [-1.1247, -0.6770, -0.1321]]), tensor([[-7.2271, -2.2115,  4.6654],\n",
              "         [-5.1776, -2.9245,  3.6597],\n",
              "         [ 1.2465,  3.8694,  2.8885]]), tensor([0., 0., 0., 0., 0., 0., 0.]), tensor([[-0.0078, -0.4810,  1.7825, -0.7949, -2.4141, -0.1699,  2.0147],\n",
              "         [-0.5051, -1.1403, -0.0978, -0.4590, -2.5304,  0.6930,  0.5222],\n",
              "         [ 0.2094, -1.1392, -0.2885,  1.8018, -0.5235, -2.1882,  0.4798]]), tensor([[  2.1500,   0.6831, -14.0124,  15.1662,  20.6007, -10.5133, -13.4766],\n",
              "         [  2.2837,   1.6561,  -9.9992,  12.0521,  17.9836,  -9.1549, -10.2023],\n",
              "         [ -1.3593,  -8.3024,   1.0103,   2.4378, -14.3125,  -3.8511,   5.9177]]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# получить матрицу 4*6\n",
        "# 4*3 * 3*3= 4*3 * 3*6 = 4*6"
      ],
      "metadata": {
        "id": "7tXuw1x8HaN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = inputs.T\n",
        "linear_1 = Linear(n_features = 3, n_neurons = 3)\n",
        "linear_2 = Linear(n_features = 3, n_neurons = 6)\n",
        "outs1 = linear_1.forward(inputs)\n",
        "outs2 = linear_2.forward(outs1)\n",
        "outs1, outs2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL2UFaxBHwor",
        "outputId": "ae682599-631c-40cb-b7ee-8e69a6672f34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ -3.1390,  -1.5427,  -1.7559],\n",
              "         [-13.5502,   6.8016, -14.9879],\n",
              "         [ -6.8784,   7.9821,  -4.9464],\n",
              "         [ -6.3612,   0.8737,  -3.5931]]),\n",
              " tensor([[  5.3171,  -3.0062,   4.1929,  -5.4196,  -1.4758,   0.8733],\n",
              "         [ 47.8256, -50.7850,  22.8687, -11.0567,   2.3485,  13.4827],\n",
              "         [ 28.6504, -28.7256,   8.0045,   7.1356,   3.6923,   8.7480],\n",
              "         [ 16.4864, -13.2178,   7.7246,  -3.7029,  -0.5820,   4.0824]]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRVH_2K7xTBC"
      },
      "source": [
        "## 2.2 Создание функций активации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9kngE6Fxs9D"
      },
      "source": [
        "2.2.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ReLU:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/f4353f4e3e484130504049599d2e7b040793e1eb)\n",
        "\n",
        "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZLvMRByxSTC"
      },
      "outputs": [],
      "source": [
        "class ReLU:\n",
        "  def forward(self, inputs):\n",
        "    # <реализовать логику ReLU>\n",
        "    inputs[inputs<0] = 0\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.randn(4, 3)\n",
        "print(inputs) \n",
        "ReLU().forward(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZksKk-yXphMl",
        "outputId": "0bea44df-65d0-46a5-8795-36016a0a91b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.5714, -0.7370,  1.8905],\n",
            "        [-0.1998,  2.0304, -0.1296],\n",
            "        [ 1.1590,  0.1874, -1.0878],\n",
            "        [-0.2569,  0.7427, -0.0921]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.0000, 1.8905],\n",
              "        [0.0000, 2.0304, 0.0000],\n",
              "        [1.1590, 0.1874, 0.0000],\n",
              "        [0.0000, 0.7427, 0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puExCWiKyTtb"
      },
      "source": [
        "2.2.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации softmax:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/6d7500d980c313da83e4117da701bf7c8f1982f5)\n",
        "\n",
        "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации. Строки матрицы трактовать как выходы линейного слоя некоторого классификатора для 4 различных примеров."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXNcFlqqyKHl"
      },
      "outputs": [],
      "source": [
        "class Softmax:\n",
        "  def forward(self, inputs):\n",
        "    # <реализовать логику Softmax>\n",
        "    return torch.exp(inputs) / torch.exp(inputs).sum(1).view(-1, 1)  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.randn(4, 3)\n",
        "print(inputs)\n",
        "Softmax().forward(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1QhH2HirOqi",
        "outputId": "430cc98e-efa3-464d-b797-dcc82a690ae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.9549,  1.0250,  0.7244],\n",
            "        [-1.7809, -1.5076, -0.5768],\n",
            "        [-0.0043,  0.7930, -2.1991],\n",
            "        [ 1.0548, -0.7455, -1.2421]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0106, 0.5685, 0.4209],\n",
              "        [0.1770, 0.2327, 0.5903],\n",
              "        [0.3002, 0.6664, 0.0334],\n",
              "        [0.7900, 0.1306, 0.0794]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxVK2TYez_Ye"
      },
      "source": [
        "2.2.3 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ELU:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/eb23becd37c3602c4838e53f532163279192e4fd)\n",
        "\n",
        "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzMz7HDLySxK"
      },
      "outputs": [],
      "source": [
        "class ELU:\n",
        "  def __init__(self, alpha=1):\n",
        "    # <создать атрибут объекта alpha>\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # <реализовать логику ReLU>\n",
        "    inputs[inputs<0] = self.alpha * (torch.exp(inputs[inputs<0]) - 1)\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.randn(4, 3)\n",
        "print(inputs)\n",
        "ELU().forward(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCLzMQmqtNjH",
        "outputId": "82ccaa83-686d-44df-fa3f-e445ba0b572d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.8220, -1.1421,  0.4565],\n",
            "        [ 0.4525, -0.2106, -0.6334],\n",
            "        [-2.1573, -0.4335,  0.9761],\n",
            "        [ 1.5292, -1.5153, -0.7994]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.5605, -0.6809,  0.4565],\n",
              "        [ 0.4525, -0.1899, -0.4692],\n",
              "        [-0.8844, -0.3517,  0.9761],\n",
              "        [ 1.5292, -0.7803, -0.5504]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0peh8r-20Pof"
      },
      "source": [
        "## 2.3 Создание функции потерь"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY-k3eEs0f7f"
      },
      "source": [
        "2.3.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь MSE:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e258221518869aa1c6561bb75b99476c4734108e)\n",
        "\n",
        "Создать полносвязный слой с 1 нейроном, прогнать через него батч `inputs` и посчитать значение MSE, трактуя вектор `y` как вектор правильных ответов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9-wdj5Tz-br"
      },
      "outputs": [],
      "source": [
        "class MSELoss:\n",
        "  def forward(self, y_pred, y_true):\n",
        "    # <реализовать логику MSE>\n",
        "    return 1 / y_pred.size()[0] * ((y_true - y_pred)**2).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAyuDU9F1Vuz"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
        "                       [2, 5, -1, 2], \n",
        "                       [-1.5, 2.7, 3.3, -0.8]])\n",
        "\n",
        "y = torch.tensor([2, 3, 4])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = Linear(n_features=4, n_neurons=1).forward(inputs)\n",
        "y_pred, MSELoss().forward(y_pred, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt3HOHPDuie9",
        "outputId": "4d44e7b9-86c8-47dc-e5f2-6080b8c0024c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-11.5559],\n",
              "         [ -5.0041],\n",
              "         [ -8.2320]]), tensor(404.0987))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaR7rILd1eWR"
      },
      "source": [
        "2.3.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь Categorical Cross-Entropy:\n",
        "\n",
        "<img src=\"https://i.ibb.co/93gy1dN/Screenshot-9.png\" width=\"200\">\n",
        "\n",
        "Создать полносвязный слой с 3 нейронами и прогнать через него батч `inputs`. Полученный результат пропустить через функцию активации softmax. Посчитать значение CCE, трактуя вектор `y` как вектор правильных ответов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQl8pJsT3HcF"
      },
      "outputs": [],
      "source": [
        "# НЕПРАВИЛЬНО!!! убрать 1 из сум - должно же быть одно число (меня смутило суммирование по j?)\n",
        "class CategoricalCrossentropyLoss:\n",
        "  def forward(self, y_pred, y_true):\n",
        "    # <реализовать логику CCE>\n",
        "    return -1 * (y_true * y_pred.log()).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7Qoupfo1ZGJ"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
        "                        [2, 5, -1, 2], \n",
        "                        [-1.5, 2.7, 3.3, -0.8]])\n",
        "y = torch.tensor([1, 0, 0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = Linear(n_features=4, n_neurons=3).forward(inputs)\n",
        "CategoricalCrossentropyLoss().forward(Softmax().forward(y_pred) , y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzbJujlTEKaW",
        "outputId": "226ce1b1-2f08-4a17-9371-d0c4476c9ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1964)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA6dbanf44_4"
      },
      "source": [
        "2.3.3 Модифицировать 2.3.1, добавив L2-регуляризацию.\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/d92ca2429275bfdc0474523babbafe014ca8b580)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADsZxD-h4_Os"
      },
      "outputs": [],
      "source": [
        "# class MSELossL2:\n",
        "#   def __init__(self, lambda_):\n",
        "#     # <создать атрибут объекта alpha>\n",
        "#     self.alpha = lambda_\n",
        "\n",
        "#   def data_loss(self, y_pred, y_true):\n",
        "#     # <подсчет первого слагаемого из формулы>\n",
        "#     return ((y_true - y_pred)**2).sum()\n",
        "\n",
        "#   def reg_loss(self, layer):\n",
        "#     # используйте атрибуты объекта layer, в которых хранятся веса слоя\n",
        "#     # <подсчет второго слагаемого из формулы>\n",
        "#     return self.alpha * (layer.weights**2).sum()\n",
        "\n",
        "#   def forward(self, y_pred, y_true):\n",
        "#     return self.data_loss(y_pred, y_true) + self.reg_loss(y_pred, y_true)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a - weights\n",
        "class MSELossL2:\n",
        "  def __init__(self, lambda_, layer):\n",
        "    # <создать атрибут объекта alpha>\n",
        "    self.alpha = lambda_\n",
        "    self.layer = layer\n",
        "\n",
        "  def data_loss(self, y_pred, y_true):\n",
        "    # <подсчет первого слагаемого из формулы>\n",
        "    return ((y_true - y_pred)**2).sum()\n",
        "\n",
        "  def reg_loss(self, layer):\n",
        "    # используйте атрибуты объекта layer, в которых хранятся веса слоя\n",
        "    # <подсчет второго слагаемого из формулы>\n",
        "    return self.alpha * (layer.weights**2).sum()\n",
        "\n",
        "  def forward(self, y_pred, y_true):\n",
        "    return self.data_loss(y_pred, y_true) + self.reg_loss(self.layer)"
      ],
      "metadata": {
        "id": "HPVE9Rp5Hz3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = Linear(n_features=4, n_neurons=3)\n",
        "y_pred, MSELossL2(lambda_=1, layer=layer).forward(layer.forward(inputs), y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf2IXq2QG_uc",
        "outputId": "71a939e6-1060-4f8b-b00c-ec4b00cdb9bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-5.1654,  1.5482, -1.2737],\n",
              "         [-7.9207,  8.8930,  1.3222],\n",
              "         [-0.9662, -3.1817, -5.2910]]), tensor(261.5272))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w049ZSdR6qQi"
      },
      "source": [
        "## 2.4 Обратное распространение ошибки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBtCfSME9W7Q"
      },
      "source": [
        "2.4.1 Используя один нейрон и SGD (1 пример за шаг), решите задачу регрессии"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xmI-QJ66WAF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#генерация датасета для задачи регрессии\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5)\n",
        "X = torch.from_numpy(X).to(dtype=torch.float32)\n",
        "y = torch.from_numpy(y).to(dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpPSPYSpD9Ey"
      },
      "source": [
        "[Граф вычислений для этой задачи](https://i.ibb.co/2dhDxZx/photo-2021-02-15-17-18-04.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc1sXtGd_J-y"
      },
      "source": [
        "2.4.1.1 Реализуйте класс `SquaredLoss`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llFigkqd_JRU"
      },
      "outputs": [],
      "source": [
        "class SquaredLoss:\n",
        "    def forward(self, y_pred, y_true):\n",
        "      return ((y_pred - y_true) ** 2).mean() # <реализовать логику MSE> - функция потерь\n",
        "\n",
        "    def backward(self, y_pred, y_true):\n",
        "      self.dinput = 2 * (y_pred - y_true).mean() # df/dc - dvalue\n",
        "      return self.dinput"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY7ForfM97UQ"
      },
      "source": [
        "2.4.1.2. Модифицируйте класс `Neuron` из __2.1.1__:\n",
        "\n",
        "  1) Сделайте так, чтобы веса нейрона инициализировались из стандартного нормального распределения\n",
        "\n",
        "  2) Реализуйте расчет градиента относительно весов `weights` и `bias`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron:\n",
        "  def __init__(self, n_inputs):\n",
        "    # <создать атрибуты объекта weights и bias>\n",
        "    self.n_inputs = n_inputs\n",
        "    self.weights = torch.randn(n_inputs)\n",
        "    self.bias = torch.randn(1)\n",
        "  \n",
        "  def forward(self, input):\n",
        "    # <реализовать логику нейрона>\n",
        "    self.input = input\n",
        "    return torch.dot(self.weights, input) + self.bias\n",
        "  \n",
        "  def backward(self, dvalue):\n",
        "    # dvalue - значение производной, которое приходит нейрону от следующего слоя сети\n",
        "    # в данном случае это будет значение df/dc (созданное методом backwards у объекта MSELoss)\n",
        "    self.dweights = dvalue * self.input # df/dW     de/dt*X\n",
        "    self.dinput =  dvalue * self.weights # df/wX    de/dt*W\n",
        "    self.dbias = dvalue # df/db                     de/dt*1"
      ],
      "metadata": {
        "id": "GM5rvsxia4Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKcO4zOLACxM"
      },
      "source": [
        "2.4.1.3 Допишите цикл для настройки весов нейрона\n",
        "\n",
        "стохастический градиентный спуск\n",
        "\n",
        "[SGD](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA)\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/dda3670f8a8996a0d3bf80856bb4a166cc8db6d4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_inputs = 4 # <размерность элемента выборки >\n",
        "lr = 0.1 #  скорость обучения\n",
        "n_epoch = 100 #  количество эпох\n",
        "\n",
        "neuron = Neuron(n_inputs)\n",
        "loss = SquaredLoss()\n",
        "\n",
        "losses = []\n",
        "for epoch in range(100):\n",
        "  for x_example, y_example in zip(X, y):\n",
        "    # forward pass\n",
        "    y_pred = neuron.forward(x_example) # <прогон через нейрон>\n",
        "    curr_loss = loss.forward(y_pred, y_example) # <прогон через функцию потерь>\n",
        "    losses.append(curr_loss)\n",
        "\n",
        "    # backprop\n",
        "    loss.backward(y_pred, y_example)\n",
        "    neuron.backward(loss.dinput)\n",
        "\n",
        "    # update weights\n",
        "    neuron.weights -= lr * neuron.dweights\n",
        "    neuron.bias -= lr * neuron.dbias\n",
        "\n",
        "  print(f'epoch {epoch} mean loss {sum(losses) / len(losses)}')\n",
        "    \n",
        "    \n",
        "    # <вызов методов backward>\n",
        "    # обратите внимание на последовательность вызовов: от конца к началу\n",
        "    # <шаг оптимизации для весов (weights и bias) нейрона>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7oWW07GOKX0",
        "outputId": "47103f3d-70bd-4727-e0fc-67b030158ca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 mean loss 860.463623046875\n",
            "epoch 1 mean loss 430.2318115234375\n",
            "epoch 2 mean loss 286.8211975097656\n",
            "epoch 3 mean loss 215.11590576171875\n",
            "epoch 4 mean loss 172.09271240234375\n",
            "epoch 5 mean loss 143.4105987548828\n",
            "epoch 6 mean loss 122.92337036132812\n",
            "epoch 7 mean loss 107.55795288085938\n",
            "epoch 8 mean loss 95.60706329345703\n",
            "epoch 9 mean loss 86.04635620117188\n",
            "epoch 10 mean loss 78.22396087646484\n",
            "epoch 11 mean loss 71.7052993774414\n",
            "epoch 12 mean loss 66.18950653076172\n",
            "epoch 13 mean loss 61.46168518066406\n",
            "epoch 14 mean loss 57.36423873901367\n",
            "epoch 15 mean loss 53.77897644042969\n",
            "epoch 16 mean loss 50.61550521850586\n",
            "epoch 17 mean loss 47.803531646728516\n",
            "epoch 18 mean loss 45.28755569458008\n",
            "epoch 19 mean loss 43.02317810058594\n",
            "epoch 20 mean loss 40.974456787109375\n",
            "epoch 21 mean loss 39.11198043823242\n",
            "epoch 22 mean loss 37.411460876464844\n",
            "epoch 23 mean loss 35.8526496887207\n",
            "epoch 24 mean loss 34.41854476928711\n",
            "epoch 25 mean loss 33.09475326538086\n",
            "epoch 26 mean loss 31.869022369384766\n",
            "epoch 27 mean loss 30.73084259033203\n",
            "epoch 28 mean loss 29.671157836914062\n",
            "epoch 29 mean loss 28.682119369506836\n",
            "epoch 30 mean loss 27.75688934326172\n",
            "epoch 31 mean loss 26.889488220214844\n",
            "epoch 32 mean loss 26.07465362548828\n",
            "epoch 33 mean loss 25.30775260925293\n",
            "epoch 34 mean loss 24.584674835205078\n",
            "epoch 35 mean loss 23.901765823364258\n",
            "epoch 36 mean loss 23.255773544311523\n",
            "epoch 37 mean loss 22.64377784729004\n",
            "epoch 38 mean loss 22.063169479370117\n",
            "epoch 39 mean loss 21.51158905029297\n",
            "epoch 40 mean loss 20.98691749572754\n",
            "epoch 41 mean loss 20.487228393554688\n",
            "epoch 42 mean loss 20.010780334472656\n",
            "epoch 43 mean loss 19.55599021911621\n",
            "epoch 44 mean loss 19.12141227722168\n",
            "epoch 45 mean loss 18.705730438232422\n",
            "epoch 46 mean loss 18.307735443115234\n",
            "epoch 47 mean loss 17.92632484436035\n",
            "epoch 48 mean loss 17.560482025146484\n",
            "epoch 49 mean loss 17.209272384643555\n",
            "epoch 50 mean loss 16.871835708618164\n",
            "epoch 51 mean loss 16.54737663269043\n",
            "epoch 52 mean loss 16.23516273498535\n",
            "epoch 53 mean loss 15.934511184692383\n",
            "epoch 54 mean loss 15.644792556762695\n",
            "epoch 55 mean loss 15.365421295166016\n",
            "epoch 56 mean loss 15.095852851867676\n",
            "epoch 57 mean loss 14.835578918457031\n",
            "epoch 58 mean loss 14.584128379821777\n",
            "epoch 59 mean loss 14.341059684753418\n",
            "epoch 60 mean loss 14.105960845947266\n",
            "epoch 61 mean loss 13.87844467163086\n",
            "epoch 62 mean loss 13.65815258026123\n",
            "epoch 63 mean loss 13.444744110107422\n",
            "epoch 64 mean loss 13.23790168762207\n",
            "epoch 65 mean loss 13.03732681274414\n",
            "epoch 66 mean loss 12.842740058898926\n",
            "epoch 67 mean loss 12.653876304626465\n",
            "epoch 68 mean loss 12.470486640930176\n",
            "epoch 69 mean loss 12.292337417602539\n",
            "epoch 70 mean loss 12.119205474853516\n",
            "epoch 71 mean loss 11.950882911682129\n",
            "epoch 72 mean loss 11.787172317504883\n",
            "epoch 73 mean loss 11.627886772155762\n",
            "epoch 74 mean loss 11.472847938537598\n",
            "epoch 75 mean loss 11.32188892364502\n",
            "epoch 76 mean loss 11.174851417541504\n",
            "epoch 77 mean loss 11.031584739685059\n",
            "epoch 78 mean loss 10.89194393157959\n",
            "epoch 79 mean loss 10.755794525146484\n",
            "epoch 80 mean loss 10.623007774353027\n",
            "epoch 81 mean loss 10.49345874786377\n",
            "epoch 82 mean loss 10.36703109741211\n",
            "epoch 83 mean loss 10.243614196777344\n",
            "epoch 84 mean loss 10.123101234436035\n",
            "epoch 85 mean loss 10.005390167236328\n",
            "epoch 86 mean loss 9.890386581420898\n",
            "epoch 87 mean loss 9.777995109558105\n",
            "epoch 88 mean loss 9.668129920959473\n",
            "epoch 89 mean loss 9.56070613861084\n",
            "epoch 90 mean loss 9.455643653869629\n",
            "epoch 91 mean loss 9.352865219116211\n",
            "epoch 92 mean loss 9.252296447753906\n",
            "epoch 93 mean loss 9.153867721557617\n",
            "epoch 94 mean loss 9.057511329650879\n",
            "epoch 95 mean loss 8.963162422180176\n",
            "epoch 96 mean loss 8.870759010314941\n",
            "epoch 97 mean loss 8.780241012573242\n",
            "epoch 98 mean loss 8.691551208496094\n",
            "epoch 99 mean loss 8.604636192321777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_g_FvwvmALJd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e7c7ef7-b01f-4240-c15a-c64f676d2374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 mean loss 3178.654541015625\n",
            "epoch 1 mean loss 1782.4659423828125\n",
            "epoch 2 mean loss 1194.2769775390625\n",
            "epoch 3 mean loss 895.7516479492188\n",
            "epoch 4 mean loss 716.60986328125\n",
            "epoch 5 mean loss 597.1755981445312\n",
            "epoch 6 mean loss 511.86480712890625\n",
            "epoch 7 mean loss 447.8817138671875\n",
            "epoch 8 mean loss 398.1170654296875\n",
            "epoch 9 mean loss 358.30535888671875\n",
            "epoch 10 mean loss 325.7321472167969\n",
            "epoch 11 mean loss 298.58782958984375\n",
            "epoch 12 mean loss 275.6195068359375\n",
            "epoch 13 mean loss 255.93240356445312\n",
            "epoch 14 mean loss 238.8702392578125\n",
            "epoch 15 mean loss 223.94085693359375\n",
            "epoch 16 mean loss 210.7678680419922\n",
            "epoch 17 mean loss 199.05853271484375\n",
            "epoch 18 mean loss 188.58177185058594\n",
            "epoch 19 mean loss 179.15267944335938\n",
            "epoch 20 mean loss 170.62159729003906\n",
            "epoch 21 mean loss 162.86607360839844\n",
            "epoch 22 mean loss 155.78494262695312\n",
            "epoch 23 mean loss 149.29391479492188\n",
            "epoch 24 mean loss 143.32215881347656\n",
            "epoch 25 mean loss 137.80975341796875\n",
            "epoch 26 mean loss 132.7056884765625\n",
            "epoch 27 mean loss 127.96620178222656\n",
            "epoch 28 mean loss 123.55357360839844\n",
            "epoch 29 mean loss 119.43511962890625\n",
            "epoch 30 mean loss 115.5823745727539\n",
            "epoch 31 mean loss 111.97042846679688\n",
            "epoch 32 mean loss 108.57738494873047\n",
            "epoch 33 mean loss 105.3839340209961\n",
            "epoch 34 mean loss 102.37296295166016\n",
            "epoch 35 mean loss 99.52926635742188\n",
            "epoch 36 mean loss 96.83928680419922\n",
            "epoch 37 mean loss 94.29088592529297\n",
            "epoch 38 mean loss 91.8731689453125\n",
            "epoch 39 mean loss 89.57633972167969\n",
            "epoch 40 mean loss 87.39155578613281\n",
            "epoch 41 mean loss 85.31079864501953\n",
            "epoch 42 mean loss 83.32682800292969\n",
            "epoch 43 mean loss 81.43303680419922\n",
            "epoch 44 mean loss 79.6234130859375\n",
            "epoch 45 mean loss 77.89247131347656\n",
            "epoch 46 mean loss 76.23519134521484\n",
            "epoch 47 mean loss 74.64695739746094\n",
            "epoch 48 mean loss 73.12355041503906\n",
            "epoch 49 mean loss 71.66107940673828\n",
            "epoch 50 mean loss 70.2559585571289\n",
            "epoch 51 mean loss 68.90487670898438\n",
            "epoch 52 mean loss 67.60478210449219\n",
            "epoch 53 mean loss 66.35284423828125\n",
            "epoch 54 mean loss 65.14643096923828\n",
            "epoch 55 mean loss 63.98310089111328\n",
            "epoch 56 mean loss 62.860591888427734\n",
            "epoch 57 mean loss 61.77678680419922\n",
            "epoch 58 mean loss 60.72972106933594\n",
            "epoch 59 mean loss 59.717559814453125\n",
            "epoch 60 mean loss 58.738582611083984\n",
            "epoch 61 mean loss 57.79118728637695\n",
            "epoch 62 mean loss 56.87386703491211\n",
            "epoch 63 mean loss 55.98521423339844\n",
            "epoch 64 mean loss 55.1239013671875\n",
            "epoch 65 mean loss 54.288692474365234\n",
            "epoch 66 mean loss 53.47841262817383\n",
            "epoch 67 mean loss 52.69196701049805\n",
            "epoch 68 mean loss 51.928314208984375\n",
            "epoch 69 mean loss 51.18648147583008\n",
            "epoch 70 mean loss 50.465545654296875\n",
            "epoch 71 mean loss 49.76463317871094\n",
            "epoch 72 mean loss 49.08292770385742\n",
            "epoch 73 mean loss 48.41964340209961\n",
            "epoch 74 mean loss 47.7740478515625\n",
            "epoch 75 mean loss 47.145442962646484\n",
            "epoch 76 mean loss 46.533164978027344\n",
            "epoch 77 mean loss 45.93658447265625\n",
            "epoch 78 mean loss 45.35511016845703\n",
            "epoch 79 mean loss 44.788169860839844\n",
            "epoch 80 mean loss 44.2352294921875\n",
            "epoch 81 mean loss 43.695777893066406\n",
            "epoch 82 mean loss 43.16931915283203\n",
            "epoch 83 mean loss 42.655399322509766\n",
            "epoch 84 mean loss 42.15357208251953\n",
            "epoch 85 mean loss 41.663414001464844\n",
            "epoch 86 mean loss 41.18452453613281\n",
            "epoch 87 mean loss 40.71651840209961\n",
            "epoch 88 mean loss 40.259029388427734\n",
            "epoch 89 mean loss 39.81170654296875\n",
            "epoch 90 mean loss 39.37421417236328\n",
            "epoch 91 mean loss 38.94623565673828\n",
            "epoch 92 mean loss 38.52745819091797\n",
            "epoch 93 mean loss 38.11759567260742\n",
            "epoch 94 mean loss 37.71635818481445\n",
            "epoch 95 mean loss 37.32347869873047\n",
            "epoch 96 mean loss 36.93870162963867\n",
            "epoch 97 mean loss 36.56177520751953\n",
            "epoch 98 mean loss 36.19246292114258\n",
            "epoch 99 mean loss 35.83053970336914\n"
          ]
        }
      ],
      "source": [
        "# TODO: берем рандомную часть выборки, каждый раз разная; \n",
        "# считать ошибку не на всем датасете а только на рандомной части\n",
        "# batch - рандомная часть выборки\n",
        "\n",
        "n_inputs = 4 # <размерность элемента выборки >\n",
        "lr = 0.1 #  скорость обучения\n",
        "n_epoch = 100 #  количество эпох\n",
        "batch_size = 15\n",
        "\n",
        "neuron = Neuron(n_inputs)\n",
        "loss = SquaredLoss()\n",
        "\n",
        "losses = []\n",
        "for epoch in range(100):\n",
        "  sample = torch.randint(0, X.shape[0], size=(batch_size,)) # берем рандомные индексы\n",
        "  for x_example, y_example in zip(X[sample], y[sample]):\n",
        "    # forward pass\n",
        "    y_pred = neuron.forward(x_example) # <прогон через нейрон>\n",
        "    curr_loss = loss.forward(y_pred, y_example) # <прогон через функцию потерь>\n",
        "    losses.append(curr_loss)\n",
        "\n",
        "    # backprop\n",
        "    loss.backward(y_pred, y_example)\n",
        "    neuron.backward(loss.dinput)\n",
        "\n",
        "    # update weights\n",
        "    neuron.weights -= lr * neuron.dweights\n",
        "    neuron.bias -= lr * neuron.dbias\n",
        "\n",
        "  print(f'epoch {epoch} mean loss {np.mean(losses)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5fOGI_X2UORI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}