{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "blank__06_CNN_embeddings_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtFQP3RNll3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2335f3c3-4506-489f-dee0-ae3136b30053"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aszp7KbOstY",
        "outputId": "70fcaf74-660f-45b9-cb65-352a603b23c2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D6VU-qbO68-",
        "outputId": "76b09aa4-8f1e-4db6-e071-0c6f31a07158"
      },
      "source": [
        "cd drive/MyDrive/datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx75RigN8xIJ"
      },
      "source": [
        "## 1. Представление и предобработка текстовых данных в виде последовательностей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LScKIAey9dAM"
      },
      "source": [
        "1.1 Представьте первое предложение из строки `text` как последовательность из индексов слов, входящих в это предложение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phEw721T9SYW"
      },
      "source": [
        "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSFQCPtD9x5J"
      },
      "source": [
        "1.2 Представьте первое предложение из строки `text` как последовательность векторов, соответствующих индексам слов. Для представления индекса в виде вектора используйте унитарное кодирование. В результате должен получиться двумерный тензор размера `количество слов в предложении` x `количество уникальных слов`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZS4XLV0-buf"
      },
      "source": [
        "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZvQKHYA-mJN"
      },
      "source": [
        "1.3 Решите задачу 1.2, используя модуль `nn.Embedding`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXjM7qEUNFY_"
      },
      "source": [
        "## 2. Классификация фамилий по национальности (ConvNet)\n",
        "\n",
        "Датасет: https://disk.yandex.ru/d/owHew8hzPc7X9Q?w=1\n",
        "\n",
        "2.1 Считать файл `surnames/surnames.csv`. \n",
        "\n",
        "2.2 Закодировать национальности числами, начиная с 0.\n",
        "\n",
        "2.3 Разбить датасет на обучающую и тестовую выборку\n",
        "\n",
        "2.4 Реализовать класс `Vocab` (токен = __символ__)\n",
        "  * добавьте в словарь специальный токен `<PAD>` с индексом 0\n",
        "  * при создании словаря сохраните длину самой длинной последовательности из набора данных в виде атрибута `max_seq_len`\n",
        "\n",
        "2.5 Реализовать класс `SurnamesDataset`\n",
        "  * метод `__getitem__` возвращает пару: <последовательность индексов токенов (см. 1.1 ), номер класса> \n",
        "  * длина каждой такой последовательности должна быть одинаковой и равной `vocab.max_seq_len`. Чтобы добиться этого, дополните последовательность справа индексом токена `<PAD>` до нужной длины\n",
        "\n",
        "2.6. Обучить классификатор.\n",
        "  \n",
        "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`. Рассмотрите два варианта: \n",
        "    - когда токен представляется в виде унитарного вектора и модуль `nn.Embedding` не обучается\n",
        "    - когда токен представляется в виде вектора небольшой размерности (меньше, чем размер словаря) и модуль `nn.Embedding` обучается\n",
        "\n",
        "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
        "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`\n",
        "\n",
        "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: прогнать несколько фамилий студентов группы через модели и проверить результат. Для каждой фамилии выводить 3 наиболее вероятных предсказания."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGfJX2NP1sw4"
      },
      "source": [
        "class Vocab:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHjCRqQg1sw5"
      },
      "source": [
        "class SurnamesDataset(Dataset):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo-hf5CQ0iWv"
      },
      "source": [
        "## 3. Классификация обзоров на фильмы (ConvNet)\n",
        "\n",
        "Датасет: https://disk.yandex.ru/d/tdinpb0nN_Dsrg\n",
        "\n",
        "2.1 Создайте набор данных на основе файлов polarity/positive_reviews.csv (положительные отзывы) и polarity/negative_reviews.csv (отрицательные отзывы). Разбейте на обучающую и тестовую выборку.\n",
        "  * токен = __слово__\n",
        "  * данные для обучения в датасете представляются в виде последовательности индексов токенов\n",
        "  * словарь создается на основе _только_ обучающей выборки. Для корректной обработки ситуаций, когда в тестовой выборке встретится токен, который не хранится в словаре, добавьте в словарь специальный токен `<UNK>`\n",
        "  * добавьте предобработку текста\n",
        "\n",
        "2.2. Обучите классификатор.\n",
        "  \n",
        "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding` \n",
        "    - подберите адекватную размерность вектора эмбеддинга: \n",
        "    - модуль `nn.Embedding` обучается\n",
        "\n",
        "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
        "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`\n",
        "\n",
        "\n",
        "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n",
        "* Целевое значение accuracy на валидации - 70+%"
      ]
    }
  ]
}